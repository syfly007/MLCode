{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#中文文本的分类\" data-toc-modified-id=\"中文文本的分类-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>中文文本的分类</a></div><div class=\"lev2 toc-item\"><a href=\"#文本挖掘和文本分类的概念\" data-toc-modified-id=\"文本挖掘和文本分类的概念-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>文本挖掘和文本分类的概念</a></div><div class=\"lev2 toc-item\"><a href=\"#文本分类项目\" data-toc-modified-id=\"文本分类项目-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>文本分类项目</a></div><div class=\"lev3 toc-item\"><a href=\"#文本预处理\" data-toc-modified-id=\"文本预处理-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>文本预处理</a></div><div class=\"lev4 toc-item\"><a href=\"#选择文本处理的范围\" data-toc-modified-id=\"选择文本处理的范围-1211\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>选择文本处理的范围</a></div><div class=\"lev4 toc-item\"><a href=\"#建立分类文本语料库\" data-toc-modified-id=\"建立分类文本语料库-1212\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>建立分类文本语料库</a></div><div class=\"lev4 toc-item\"><a href=\"#文本格式转换\" data-toc-modified-id=\"文本格式转换-1213\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;</span>文本格式转换</a></div><div class=\"lev4 toc-item\"><a href=\"#检测句子边界：标记句子的结束\" data-toc-modified-id=\"检测句子边界：标记句子的结束-1214\"><span class=\"toc-item-num\">1.2.1.4&nbsp;&nbsp;</span>检测句子边界：标记句子的结束</a></div><div class=\"lev3 toc-item\"><a href=\"#中文分词介绍\" data-toc-modified-id=\"中文分词介绍-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>中文分词介绍</a></div><div class=\"lev3 toc-item\"><a href=\"#Scikit-Learn-库简介\" data-toc-modified-id=\"Scikit-Learn-库简介-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Scikit-Learn 库简介</a></div><div class=\"lev3 toc-item\"><a href=\"#向量空间模型\" data-toc-modified-id=\"向量空间模型-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>向量空间模型</a></div><div class=\"lev3 toc-item\"><a href=\"#权重策略:TF-IDF-方法\" data-toc-modified-id=\"权重策略:TF-IDF-方法-125\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>权重策略:TF-IDF 方法</a></div><div class=\"lev3 toc-item\"><a href=\"#使用朴素贝叶斯分类模块\" data-toc-modified-id=\"使用朴素贝叶斯分类模块-126\"><span class=\"toc-item-num\">1.2.6&nbsp;&nbsp;</span>使用朴素贝叶斯分类模块</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章以实例的形式，系统的介绍了中文文本分类的整体流程和相关算法。内容从文本挖掘的大背景开始，以文本分类算法为中心，详细介绍了一个中文文本分类项目的流程及其周边方方面面的知识，知识点涉及中文分词、向量空间模型、TF-IDF 方法直到几个典型的文本分类算法和评价指标等等。因为是第一章，为便于读者的学习，所使用的算法技术并不复杂：\n",
    ">* 朴素的贝叶斯算法\n",
    ">* KNN 最近邻算法\n",
    "\n",
    "代码讲解分为两大部分：\n",
    ">* 外部库如 jieba 分词、Scikit-Learning：提供详细的使用说明和案例代码\n",
    ">* 算法：提供详细的讲解和注释\n",
    "与第一章相同，本章使用矢量编程，代码结构较很简单。阅读起来一目了然。通过学习本章，读者有能力实现小型的文本分类系统，并应用于实践之中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本挖掘和文本分类的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本挖掘（Text Mining）:文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参考。\n",
    "\n",
    "文本挖掘的七个主要领域：\n",
    ">* 搜索和信息检索（IR）：存储和文本文档的检索，包括搜索引擎和关键字搜索。\n",
    ">* 文本聚类：使用聚类方法，对词汇，片段，段落或文件进行分组和归类。\n",
    ">* 文本分类：对片段，段落或文件进行分组和归类，使用数据挖掘分类方法的基础上，经过训练的标记示例模型。\n",
    ">* Web 挖掘：在互联网上进行数据和文本挖掘，并特别关注在网络的规模和相互联系。\n",
    ">* 信息抽取（IE）：从非结构化文本中识别与提取有关的事实和关系;从非结构化和半结构化文本制作的结构化数据的过程。\n",
    ">* 自然语言处理（NLP）：将语言作为一种有意义、有规则的符号系统，在底层解析和理解语言的任务（例如，词性标注）;目前的技术主要从语法、语义的角度发现语言最本质的结构和所表达的意义。\n",
    ">* 概念提取：把单词和短语按语义分组成意义相似的组。\n",
    "\n",
    "可以说在分析机器学习的数据源中最常见的知识发现主题是把数据对象或事件转换为预定的类别，再根据类别进行专门的处理，这是分类系统的基本任务。文本分类也如此：其实就是为用户给出的每个文档找到所属的正确类别（主题或概念）。\n",
    "\n",
    "目前，有两种主要的文本分类方法，一是基于模式系统（通过运用知识工程技术），二是分类模型（通过使用统计和/或机器学习技术）。专家系统的方法是将专家的知识以规则表达式的形式编码成分类系统。机器学习的方法是一个广义归纳过程，采用由一组预分类的例子，通过训练建立分类。由于文件数量以指数速度的增加和知识专家的可用性变得越来越小，潮流趋势正在转向机器学习 - 基于自动分类技术。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节开始，我们和读者一起完成一个文本分类项目的全过程。虽然从分类算法层面来看，各类语言的文本分类技术都大同小异。但从整个流程来考察，不同语言的文本处理所用到的技术还是有差别的。下面给出中文语言的文本分类技术和流程，主要包括以下几个步骤：\n",
    "1. 预处理：去除文本的噪声信息，例如 HTML 标签，文本格式转换，检测句子边界等等；\n",
    "2. 中文分词：使用中文分词器为文本分词，并去除停用词；\n",
    "3. 构建词向量空间：统计文本词频，生成文本的词向量空间；\n",
    "4. 权重策略--TF-IDF 方法：使用 TF-IDF 发现特征词，并抽取为反映文档主题的特征；\n",
    "5. 分类器：使用算法训练分类器；\n",
    "6. 评价分类结果：分类器的测试结果分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "基本步骤：\n",
    "#### 选择文本处理的范围\n",
    "#### 建立分类文本语料库\n",
    "\n",
    "    文本语料一般分为2大类：\n",
    "    **训练集语料**:目前比较好的中文分类语料库有：\n",
    "    - 复旦大学谭松波中文文本分类语料(下载地址:http://www.threedweb.cn/thread-1292-1-1.html)\n",
    "    - 搜狗新闻分类语料库（下载地址：http://www.sogou.com/labs/dl/c.html）\n",
    "\n",
    "    中文文本分类语料下载地址：http://www.threedweb.cn/thread-1288-1-1.html\n",
    "\n",
    "\n",
    "    **测试集语料**\n",
    "\n",
    "    所谓测试集就是待分类的文本语料，可以是训练集的一部分，也可以是外部来源的文本语料。外部来源比较自由，一般实际项目都是解决新文本的分类问题。\n",
    "#### 文本格式转换\n",
    "    例如html，其表格<table></table>内的信息一般是结构化的。对于基于机器学习的分类系统没什么用，但是对于基于模式的系统却很有价值，过滤掉这些有意义的标签之后，就要去除 html 的其余所有标签，将文本转换为 txt格式或 xml 格式的半结构化文本。最容易出错的地方不是字符集的不同：Python 可以通过 Encode，Decode，Unicode 等指令，自动统一将文档编码转换为指定的编码格式：UTF-8 或 GBK。而主要问题是乱码：一些文档在发布后，由于编辑问题或网络传输的问题，文档中常出现乱码。这类情况如果处理不好，很容易造成程序异常。\n",
    "    为了提高性能，一般 Python 去除 html 标签，较多的使用 lxml 库，是一个 C 编写的 xml 扩展库.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 百度一下，你就知道                     新闻 hao123 地图 视频 贴吧  登录  document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">登录</a>'); 更多产品       关于百度 About Baidu  ©2016 Baidu 使用百度前必读  意见反馈 京ICP证030173号        \n"
     ]
    }
   ],
   "source": [
    "from lxml import etree,html\n",
    "# htm 文件路径，以及读取文件\n",
    "path = \"1.htm\"\n",
    "content = open(path,\"rb\").read()\n",
    "page = html.document_fromstring(content) # 解析文件\n",
    "text = page.text_content() # 去除所有标签\n",
    "print text # 输出去除标签后解析结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检测句子边界：标记句子的结束\n",
    "\n",
    "    句子边界检测是分解整个文档，并转换成单独句子的过程。对于中文文本，它就是寻找像“。”“？”或“！”等标点符号，作为断句的依据。然而，随着英语的普及，也有使用“.”作为句子的结束标志。这容易与某些词语出现的缩写或缩写词的一部分混淆。如果从这里断句，容易发生错误。这种情况可以使用一些简单的启发式规则或统计分类技术，正确识别大多数句子边界。最奇葩的现象是，某些网站使用图片作为标点符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词介绍\n",
    "\n",
    "中文分词 (Chinese Word Segmentation) 指的是将一个汉字序列（句子）切分成一个一个单独的词。\n",
    "\n",
    "分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂的多、困难的多。中文分词，不仅是中文文本分类的一大问题，也是**中文自然语言处理的核心问题**之一。\n",
    "\n",
    "一般像这种最后谁也想不明白的问题，最后都交给概率论。最终完全解决中文分词的算法是基于概率图模型的**条件随机场(CRF)**。这个算法由 Lafferty 等人于 2001 年提出，估计当时设计这个算法不是为了解决中文分词问题。\n",
    "\n",
    "分词是自然语言处理中最基本、最底层的模块，分词精度对后续应用模块影响很大，纵观整个自然语言处理领域，文本或句子的结构化表示是语言处理最核心的任务。\n",
    "\n",
    "目前，文本的结构化表示简单分为四大类：\n",
    "- 词向量空\n",
    "- 间模型、\n",
    "- 主题模型、\n",
    "- 依存句法的树表示、RDF 的图表示。\n",
    "\n",
    "以上这四种文本表示都以分词为基础的。\n",
    "\n",
    "一般专业化大型的文本分类系统，为了提高精度，常常订制开发自己的分词系统。一般算法都使用 CRF，语料资源则各有不同。\n",
    "\n",
    "现在开放出来的、比较成熟的、有商用价值的分词工具有：\n",
    "- 大学张华平博士开发的中文分词系统：http://ictclas.nlpir.org/（一年免费试用权）；\n",
    "- 大的语言云系统：http://www.ltp-cloud.com/intro/(开源)，\n",
    "- Ansj的中文分词系统：http://www.nlpcn.org/（开源）等等。\n",
    "\n",
    "以上这些分词系统与 Python 整合都比较麻烦，占用资源也大。因为分词不是本书讲解的重点，为了方便，我们尽量使用小巧高效，而且原生支持 Python 的分词系统。这里我们推荐使用 **jieba** 分词，它是专门使用 Python 语言开发的分词系统，占用资源较小，常识类文档的分词精度较高。对于非专业文档绰绰有余。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.361 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 小明   1995   年 毕业 于 北京 清华大学\n",
      "Default Param: 小明   1995   年 毕业 于 北京 清华大学\n",
      "Full Mode: 小/ 明/ / 1995/ / 年/ 毕业/ 于/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "stdi,stdo,stde=sys.stdin,sys.stdout,sys.stderr\n",
    "# 设置 utf-8 unicode 环境\n",
    "reload(sys)\n",
    "sys.stdin,sys.stdout,sys.stderr=stdi,stdo,stde #防止无法打印输出\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "seg_list = jieba.cut(\"小明 1995 年毕业于北京清华大学\", cut_all=False)\n",
    "print \"Default Mode:\", \" \".join(seg_list) # 默认切分\n",
    "seg_list = jieba.cut(\"小明 1995 年毕业于北京清华大学\")\n",
    "print \"Default Param:\", \" \".join(seg_list)\n",
    "seg_list = jieba.cut(\"小明 1995 年毕业于北京清华大学\", cut_all=True)\n",
    "print \"Full Mode:\", \"/ \".join(seg_list) # 全切分\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") # 搜索引擎模式\n",
    "print \"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通过分词，中文文本实现了最基本的结构化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文语料分词结束了！\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import shutil\n",
    "\n",
    "#1.设置字符集，并导入 jieba 分词包\n",
    "stdi,stdo,stde=sys.stdin,sys.stdout,sys.stderr\n",
    "# 设置 utf-8 unicode 环境\n",
    "reload(sys)\n",
    "sys.stdin,sys.stdout,sys.stderr=stdi,stdo,stde #防止无法打印输出\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "#2.定义创建两个函数，用于处理读取和保存文件\n",
    "def savefile(savepath,content):# 保存至文件 \n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content) \n",
    "    fp.close()\n",
    "    \n",
    "def readfile(path): # 读取文件\n",
    "    fp = open(path,\"rb\") \n",
    "    content = fp.read() \n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "#3.接上面的部分，以下是整个语料库的分词主程序:\n",
    "corpus_path = \"train_corpus_small/\" # 未分词分类语料库路径\n",
    "seg_path=\"train_corpus_seg/\" # 分词后分类语料库路径\n",
    "catelist=os.listdir(corpus_path) #获取corpus_path所有子目录\n",
    "\n",
    "#清理\n",
    "if os.path.exists(seg_path):\n",
    "    shutil.rmtree(seg_path)\n",
    "\n",
    "#获取每个子目录的所有文件\n",
    "for mydir in catelist:\n",
    "    class_path = corpus_path + mydir + \"/\" #拼接出每个子目录的路径\n",
    "    seg_dir = seg_path + mydir + \"/\" #拼接出分词后子目录的路径\n",
    "    if not os.path.exists(seg_dir):\n",
    "        os.makedirs(seg_dir)\n",
    "    file_list = os.listdir(class_path) #获取类别目录的所有文件\n",
    "    for file_path in file_list:\n",
    "        fullname=class_path+file_path\n",
    "        content=readfile(fullname).strip()#读取内容\n",
    "        content=content.replace(\"\\r\\n\",\"\").strip()#删除换行和多余的空格\n",
    "        content_seg=jieba.cut(content)#开始分词\n",
    "        savefile(seg_dir+file_path,\" \".join(content_seg))#保存至分词后的语料库\n",
    "    \n",
    "print \"中文语料分词结束了！\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词前：11.TXT\n",
    "\n",
    "【 日  期 】19960426\n",
    "【 版  号 】10\n",
    "【 标  题 】国产微机重获市场主要份额\n",
    "【 作  者 】尹容\n",
    "【 正  文 】\n",
    "    近年来受到进口微机严重冲击的国产微机，１９９５年打了个翻身仗，总销售额达\n",
    "６１５亿元，销量首次突破百万台大关，达１１５万台，从而以微弱优势重新获得了我\n",
    "国微机市场的主要份额———５０·４％的市场占有率。这是由电子部计算机与微电子\n",
    "发展研究中心（ＣＣＩＤ）近日发布的。\n",
    "    ＣＣＩＤ发布的这份中国计算机市场的权威报告表明，“八五”期间，我国计算机\n",
    "产业，呈高速发展态势，１９９０年中国本地计算机信息产业产值规模仅５０亿元，去\n",
    "年已达６９８亿元，年平均增长速度达６９·５％；１９９０年微机产量不足１０万台\n",
    "，而去年仅长城集团一家产量就逾１０万台，国内微机总产量已达１３０多万台；出口\n",
    "量也大幅度增长，去年达４９·６亿美元，是１９９０年的１３·３倍，说明我国微机\n",
    "产品在质量、性能上已被国际市场所接受，标志着中国市场已成为世界微机市场的重要\n",
    "组成部分。    （尹  容）\n",
    "\n",
    "分词后：11.TXT\n",
    "\n",
    " 【   日     期   】 19960426 【   版     号   】 10 【   标     题   】 国产 微机 重获 市场 主要 份额 【   作     者   】 尹容 【   正     文   】         近年来 受到 进口 微机 严重 冲击 的 国产 微机 ， １ ９ ９ ５ 年 打了个 翻身仗 ， 总 销售额 达 ６ １ ５ 亿元 ， 销量 首次 突破 百万 台 大关 ， 达 １ １ ５ 万台 ， 从而 以 微弱 优势 重新 获得 了 我国 微机 市场 的 主要 份额 — — — ５ ０ · ４ ％ 的 市场占有率 。 这 是 由 电子部 计算机 与 微电子 发展 研究 中心 （ Ｃ Ｃ Ｉ Ｄ ） 近日 发布 的 。         Ｃ Ｃ Ｉ Ｄ 发布 的 这份 中国 计算机 市场 的 权威 报告 表明 ， “ 八五 ” 期间 ， 我国 计算机 产业 ， 呈 高速 发展 态势 ， １ ９ ９ ０ 年 中国 本地 计算机信息 产业 产值 规模 仅 ５ ０ 亿元 ， 去年 已达 ６ ９ ８ 亿元 ， 年 平均 增长速度 达 ６ ９ · ５ ％ ； １ ９ ９ ０ 年 微机 产量 不足 １ ０ 万台 ， 而 去年 仅 长城 集团 一家 产量 就 逾 １ ０ 万台 ， 国内 微机 总产量 已达 １ ３ ０ 多万台 ； 出口量 也 大幅度 增长 ， 去年 达 ４ ９ · ６ 亿美元 ， 是 １ ９ ９ ０ 年 的 １ ３ · ３ 倍 ， 说明 我国 微机 产品 在 质量 、 性能 上 已 被 国际 市场 所 接受 ， 标志 着 中国 市场 已 成为 世界 微机 市场 的 重要 组成部分 。         （ 尹     容 ）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，在实际应用中，为了后 续生成向量空间模 型的方便，这些分词 后的文本信 息还要转换为文本向量信息并对象化。这里需要引入一个 Scikit-Learning 库的 **Bunch** 数据结构:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将分好词的文本文件转换并持久化为Bunch形式的代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建文本对象结束\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# Bunch 类提供一种 key,value 的对象形式 \n",
    "# target_name:所有分类集名称列表\n",
    "# label:每个文件的分类标签列表 \n",
    "# filenames:文件路径\n",
    "# contents:分词后文件词向量形式\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[])\n",
    "\n",
    "wordbag_path = \"train_word_bag/train_set.dat\" #分词语料Bunch对象持久化语料路径\n",
    "seg_path = \"train_corpus_seg/\" #分词后分类语料库路径\n",
    "\n",
    "#清理\n",
    "if os.path.exists(wordbag_path):\n",
    "    shutil.rmtree(\"train_word_bag\")\n",
    "os.makedirs(\"train_word_bag\")\n",
    "os.mknod(wordbag_path) #建立空文件train_set.dat\n",
    "\n",
    "catelist = os.listdir(seg_path)\n",
    "bunch.target_name.extend(catelist) #将类别信息保存到Bunch对象中\n",
    "\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path + mydir + \"/\"\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        bunch.label.append(mydir) #保存当前文件的分类标签\n",
    "        bunch.filenames.append(fullname) #保存当前文件的文件路径\n",
    "        bunch.contents.append(readfile(fullname).strip()) #保存文件向量词\n",
    "    \n",
    "#bunch对象持久化\n",
    "file_obj=open(wordbag_path,\"wb\")\n",
    "pickle.dump(bunch,file_obj) #将对象obj保存到文件file中去\n",
    "file_obj.close()\n",
    "        \n",
    "print \"构建文本对象结束\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn 库简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面的部分，将使用Scikit-Learn库中的算法模块处理生成的训练集文本向量，在讲解之前，我们先简单介绍一下 Scikit-Learn算法库。\n",
    "\n",
    "Scikit-Learn 网站的主页截图(http://scikit-learn.org/stable/) 。 Scikit-Learn 是一个用于机器学习的 Python库，建立在SciPy基础之上，获得 3-Clause BSD 开源许可证。这个网站是由David Cournapeau在2007年发起的一个Google Summer of Code项目，从那时起这个项目就已经拥有很多的贡献者了，而且目前该网站也是由一志愿者团队在维护着。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模块分类：**\n",
    "- 分类和回归算法:广义线性模型，线性和二次判别分析，岭回归，支持向量KNN，高斯过程，交叉分解，朴素贝叶斯，决策树，集 成方法，多细粒度的算法，特征选择，半监督，保序回归，概率校准;\n",
    "- 聚类算法:K-means，仿射传播，均值漂移，谱聚类，分层聚类，DBSCAN， Birch;\n",
    "- 维度约简:PCA，潜在语义分析(截断奇异值分解)，字典学习，因子分析， ICA，非负矩阵分解;\n",
    "- 模型选择:交叉验证，评价估计性能，网格搜索:测质量的量化评价，模型的持久化，验证曲线:绘制分数评价模型;\n",
    "- 数据预处理:标准化，去除均值率和方差缩放，正规化，二值化，编码分类 特征，缺失值的插补\n",
    "\n",
    "**主要特点：**\n",
    "- 操作简单、高效的数据挖掘和数据分析\n",
    "- 无访问限制，在任何情况下可重新使用\n",
    "- 建立在 NumPy、SciPy 和 matplotlib 基础上 \n",
    "- 使用商业开源协议——BSD 许可证\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 向量空间模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量空间模型把文本表示为一 个向量，其中该向 量的每个特征表示为 文本中出现 的词。通常，把训练集中出现 的每个不同的字符串 都作为一个维度，包括 常用词、专 有词、词组和其他类型模式串，如电子邮件地址和 URL。目前，大多数文本挖掘系统， 都把文本存储为向量空间的表示，因为它便于运用 机器学习算法。这类算 法适用并能 有效处理高维空间的文本情况。\n",
    "\n",
    "但是，对于大规模文本分类，这会导致极高维的空间， 即使是中等大小的文本文件集合，向量的维度也很轻易就达到数十万维。为节省存储空间和提高 搜索效率， 在文本分类之前会自动过滤掉 某些字或词，这些字 或词即被称为停用词。 这类词一般 都是意义模糊的常用词，还有一些语气助词，通常它们对文本起不了分类特征的意义。这些停用词都是人工输入、非 自动化生成的，生成 后的停用词会形成一个 停用词表。 各类停用词表大同小异，读者可以从这个网址下载停用词表: http://www.threedweb.cn/thread-1294-1-1.html。\n",
    "\n",
    "读取停用词列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "# 1. 读取停用词表\n",
    "shutil.copy(\"hlt_stop_words.txt\",\"train_word_bag/\")\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\" \n",
    "\n",
    "stpwrdlst=readfile(stopword_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重策略:TF-IDF 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对于已经看过第一章的读者， 对向量空间模型应 该并不陌生，也就是 前文所说的 词袋模型，它将文本中的词和 模式串转换为数字， 而整个文本集也都转换 为维度相等 的词向量矩阵。假设文本仍旧是第一章的三个文本:\n",
    "   \n",
    "    文本 1:My dog ate my homework。\n",
    "    文本 2:My cat ate the sandwich。\n",
    "    文本 3:A dolphin ate the homework。\n",
    "    \n",
    "生成的词袋中不重复的词还是 9 个，注意，这里增加了词频信息:\n",
    "a (1), ate (3), cat (1), dolphin (1), dog (1), homework (2), my (3), sandwich(1), the (2).直观上，文本的词向量表示可以使用二元表示:\n",
    "\n",
    "    文本 1:0,1,0,0,1,1,1,0,0(注意，尽管“my”出现了两次，但二元向量表示中仍然是“1”)\n",
    "    文本 2:0,1,1,0,0,0,1,1,1\n",
    "    文本 3:1,1,0,1,0,1,0,0,1\n",
    "    \n",
    "这种方式的问题是忽略了一个 句子中出现多个相 同词的词频信息，我们增加这些词频信息，就变成了整型计数方式，下表是使用整形计数方式的词向量表示:\n",
    "\n",
    "    文本 1:0,1,0,0,1,1,2,0,0(请注意，“my”在句子中出现的两次)\n",
    "    文本 2:0,1,1,0,0,0,1,1,1\n",
    "    文本 3:1,1,0,1,0,1,0,0,1\n",
    "    \n",
    "接下来，我们对整型计数方式进行归一化，归一化可以避免句子长度不一致问题，便于算法计算，而且对于基于 概率算法，词频信息 就变为了概率分布，这 就是文档的 TF 信息:\n",
    "\n",
    "    文本 1:0,1/5,0,0,1/5,1/5,2/5,0,0(请注意，“my”在句子中出现的两次) \n",
    "    文本 2:0,1/5,1/5,0,0,0,1/5,1/5,1/5\n",
    "    文本 3:1/5,1/5,0,1/5,0,1/5,0,0,1/5\n",
    "    \n",
    "但是这里还有个问题，如何体现生成的词袋中的词频信息呢?\n",
    "\n",
    "原信息:a (1), ate (3), cat (1), dolphin (1), dog (1), homework (2), my (3), sandwich (1), the (2).注意:由于词袋收集了所有文档中的词，这些词的词频是针对所有文档的词频， 因此，词袋的统计基数是文档数\n",
    "词条的文档频率:\n",
    "\n",
    "a (1/3), ate (3/3), cat (1/3), dolphin (1/3), dog (1/3), homework (2/3), my (2/3), sandwich (1/3), the (2/3)\n",
    "\n",
    "词袋模型的 IDF 权重:\n",
    "\n",
    "\n",
    "IDF: a log(3/1), ate log(3/3), cat log(3/1), dolphin log(3/1), dog log(3/1), homework log (3/2), my log(3/2), sandwich log (3/1), the log (3/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF 权重策略:**\n",
    "\n",
    "计算文本的权重向量，应该选择一个有效的权重方案。最流行的方案是对 TF-IDF 权重的方法。TF-IDF 的含义是**词频--逆文档频率**，其含义是如果某个词或短语在一篇文章中出现的频率 TF 高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "词频(term frequency，TF)指的是某一个给定的词语在该文件中出现的频率。这 个数字是对词数(term count)的归一化，以防止它偏向长的文件。对于在某一特定文件 里的词语来说，它的重要性可表示为:\n",
    "$$TF_{ij}=\\frac{n_{i,j}}{\\sum_{k}n_{k,j}}$$\n",
    "以上式子中分子是该词在文件 中的出现次数，而 分母则是在文件中所 有字词的出 现次数之和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆向文件频率(inverse document frequency，IDF)是一个词语普遍重要性的度量。 某一特定词语的 IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到:\n",
    "$$IDF_{i}=\\log \\dfrac {\\left| D\\right| }{\\left| \\left\\{ j:t_{i}\\in d_{j}\\right\\} \\right| }$$\n",
    "其中：\n",
    "   - |D|:语料库中的文件总数;\n",
    "   - j:包含词语的文件数目，如果该词语不在语料库中，就会导致分母为零，因此一 般情况下使用 1+|{d∈D:t∈d}|作为分母;\n",
    "   \n",
    "然后再计算 TF 与 IDF 的乘积。\n",
    "某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率， TFIDFij=TFijI×IDFij 可以产生出高权重的 TF-IDF。因此，TF-IDF 倾向于过滤掉常见 的词语，保留重要的词语。\n",
    "\n",
    "1.导入所需的 Scikit-Learn 包:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- importsys\n",
    "import os\n",
    "from sklearn.datasets.base import Bunch # 引入Bunch类\n",
    "import cPickle as pickle # 引入持久化类\n",
    "from sklearn import feature_extraction \n",
    "from sklearn.feature_extraction.text import TfidfTransformer #Tfidf向量转换类 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Tfidf 向量生成类\n",
    "\n",
    "#1.设置字符集，并导入 jieba 分词包\n",
    "stdi,stdo,stde=sys.stdin,sys.stdout,sys.stderr\n",
    "# 设置 utf-8 unicode 环境\n",
    "reload(sys)\n",
    "sys.stdin,sys.stdout,sys.stderr=stdi,stdo,stde #防止无法打印输出\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.读取和写入bunch对象的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取Bunch对象\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path,\"rb\")\n",
    "    bunch = pickle.load(file_obj)\n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "\n",
    "#写入Bunch对象\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path,\"wb\")\n",
    "    pickle.dump(bunchobj,file_obj)\n",
    "    file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.从训练集生成TF-IDF向量词带。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. 导入分词后的词向量Bunch对象\n",
    "path = \"train_word_bag/train_set.dat\"\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "# 3. 构建TF-IDF词向量空间对象\n",
    "tfidfspace=Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "\n",
    "# 4. 使用TfidfVectorizer初始化向量模型\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf=True, max_df=0.5)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "#文本转为词频矩阵，单独保存字典文件\n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.持久化TF-IDF向量词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.创建词袋的持久化\n",
    "space_path = \"train_word_bag/tfdifspace.dat\"\n",
    "writebunchobj(space_path,tfidfspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用朴素贝叶斯分类模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最常用的文本分类方法：\n",
    "- KNN最近邻：简单，精度尚可，速度最慢\n",
    "- 朴素贝叶斯：短文本分类效果最好，精度高\n",
    "- 支持向量机：支持线性不可分的情况，精度上取中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节选择 Scikit-Learn 的朴素贝叶斯算法进行文本分类，测试集随机抽取自训练集中的文档集合，每个分类取 10 个文档，过滤掉 1k 以下的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练步骤与训练集相同，首先是分词，之后生成文件词向量文件，直至生成词向量模型。不同的是在训练词向量模型时，需要加载训练集词袋，将测试集产生的词向量映射到训练集词袋的词典中，生成向量空间模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建文本对象结束\n"
     ]
    }
   ],
   "source": [
    "#1.导出分词后的词向量Bunch对象\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# Bunch 类提供一种 key,value 的对象形式 \n",
    "# target_name:所有分类集名称列表\n",
    "# label:每个文件的分类标签列表 \n",
    "# filenames:文件路径\n",
    "# contents:分词后文件词向量形式\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[])\n",
    "\n",
    "wordbag_path = \"test_word_bag/test_set.dat\"  #分词语料Bunch对象持久化语料路径\n",
    "seg_path = \"test_corpus_seg/\" #分词后分类语料库路径\n",
    "\n",
    "#清理\n",
    "if os.path.exists(wordbag_path):\n",
    "    shutil.rmtree(\"test_word_bag\")\n",
    "os.makedirs(\"test_word_bag\")\n",
    "os.mknod(wordbag_path) #建立空文件train_set.dat\n",
    "\n",
    "catelist = os.listdir(seg_path)\n",
    "bunch.target_name.extend(catelist) #将类别信息保存到Bunch对象中\n",
    "\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path + mydir + \"/\"\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        bunch.label.append(mydir) #保存当前文件的分类标签\n",
    "        bunch.filenames.append(fullname) #保存当前文件的文件路径\n",
    "        bunch.contents.append(readfile(fullname).strip()) #保存文件向量词\n",
    "    \n",
    "#bunch对象持久化\n",
    "file_obj=open(wordbag_path,\"wb\")\n",
    "pickle.dump(bunch,file_obj) #将对象obj保存到文件file中去\n",
    "file_obj.close()\n",
    "        \n",
    "print \"构建文本对象结束\"\n",
    "\n",
    "# 2.导入分词后的词向量Bunch对象\n",
    "path = \"test_word_bag/test_set.dat\"\n",
    "bunch = readbunchobj(path)\n",
    "\n",
    "#3.构建测试集tfidf向量空间\n",
    "testspace = Bunch(target_name=bunch.target_name, label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n",
    "#4.导入训练集的词袋\n",
    "trainbunch=readbunchobj(\"train_word_bag/tfdifspace.dat\")\n",
    "\n",
    "#5. 使用TfidfVectorizer初始化向量空间模型\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf=True,max_df=0.5,vocabulary=trainbunch.vocabulary)  #使用训练词袋的向量\n",
    "trainformer=TfidfTransformer()\n",
    "testspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "testspace.vocabulary = trainbunch.vocabulary\n",
    "\n",
    "#创建词袋的持久化\n",
    "space_path=\"test_word_bag/testspace.dat\"\n",
    "writebunchobj(space_path,testspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行多项式贝叶斯算法进行测试文本分类，并返回分类精度：\n",
    "\n",
    "1.导入多项式贝叶斯算法包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.执行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d4063cf02c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#预测分类结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    708\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \"\"\"\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "#导入训练集向量空间\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set=readbunchobj(trainpath)\n",
    "\n",
    "#导入测试集向量空间\n",
    "testpath=\"test_word_bag/testspace.dat\"\n",
    "test_set=readbunchobj(testpath)\n",
    "\n",
    "#应用朴素贝叶斯算法\n",
    "#alpha:0.001 alpha越小，迭代次数越多，精度越高\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "#预测分类结果\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "total = len(predicted)\n",
    "rate = 0\n",
    "for  flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate += 1\n",
    "        print file_name,\":实际类别：\",flabel,\"-->预测类别：\",expct_cate\n",
    "#预测精度\n",
    "print \"error rate:\",float(rate)*100/float(total),\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "542px",
    "left": "0px",
    "right": "1068px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
