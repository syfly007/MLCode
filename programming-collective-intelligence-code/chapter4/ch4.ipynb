{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Search And Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for full-text searches are among the most important collective intelligence algorithms, and many fortunes have been made by new ideas in this field. It is widely believed that **Google**’s rapid rise from an academic project to the world’s most popular search engine was based largely on the PageRank algorithm, a variation that you’ll learn about in this chapter.\n",
    "\n",
    "Throughout this chapter, you’ll learn all the necessary steps to crawl, index,and search a set of pages, and even rank their results in many different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s in a Search Engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in creating a search engine is to develop a way to **collect** the documents.\n",
    "In some cases, this will involve **crawling** (starting with a small set of documents and following links to others) and in other cases it will begin with a fixed collection of documents, perhaps from a corporate intranet.\n",
    "\n",
    "After you collect the documents, they need to be **indexed**. This usually involves creating a big table of the documents and the locations of all the different words.Depending on the particular application, the documents themselves do not necessarily have to be stored in a database; the index simply has to store a reference (such as a file system path or URL) to their locations.\n",
    "\n",
    "The final step is, of course, **returning** a ranked list of documents from a query.Retrieving every document with a given set of words is fairly straightforward once you have an index, but the real magic is in **how the results are sorted**.This chapter will look at several metrics based on the content of the page, such as word **frequency**, and then cover metrics based on information external to the content of the page, such as the PageRank algorithm, which looks at **how other pages link to the page in question**.\n",
    "\n",
    "To work through the examples in this chapter, you’ll need to create a Python module called searchengine, which has two classes: \n",
    "> * one for crawling and creating the database, \n",
    "> * and the other for doing full-text searches by querying the database. \n",
    "\n",
    "The examples will use SQLite, but they can easily be adapted to work with a traditional client-server database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    # Initialize the crawler with the name of database\n",
    "    def __init__(self, dbname):\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "\n",
    "    def dbcommit(self):\n",
    "        pass\n",
    "    # Auxilliary function for getting an entry id and adding\n",
    "    # it if it's not present\n",
    "\n",
    "    def getentryid(self, table, field, value, createnew=True):\n",
    "        return None\n",
    "    # Index an individual page\n",
    "\n",
    "    def addtoindex(self, url, soup):\n",
    "        print 'Indexing %s' % url\n",
    "    # Extract the text from an HTML page (no tags)\n",
    "\n",
    "    def gettextonly(self, soup):\n",
    "        return None\n",
    "    # Separate the words by any non-whitespace character\n",
    "\n",
    "    def separatewords(self, text):\n",
    "        return None\n",
    "    # Return true if this url is already indexed\n",
    "\n",
    "    def isindexed(self, url):\n",
    "        return False\n",
    "    # Add a link between two pages\n",
    "\n",
    "    def addlinkref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "    # Starting with a list of pages, do a breadth\n",
    "    # first search to the given depth, indexing pages\n",
    "    # as we go\n",
    "\n",
    "    def crawl(self, pages, depth=2):\n",
    "        pass\n",
    "    # Create the database tables\n",
    "\n",
    "    def createindextables(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll assume for now that you don’t have a big collection of HTML documents sitting on your hard drive waiting to be indexed, so I’ll show you how to build a simple crawler. It will be seeded with a small set of pages to index and will then follow any links on that page to find other pages, whose links it will also follow. This process is called crawling or spidering. \n",
    "\n",
    "To do this, your code will have to download the pages, pass them to the indexer (which you’ll build in the next section), and then parse the pages to find all the links to the pages that have to be crawled next. Fortunately, there are a couple of libraries that can help with this process. \n",
    "\n",
    "For the examples in this chapter, I have set up a copy of several thousand files from Wikipedia, which will remain static at http://kiwitobes.com/wiki. \n",
    "\n",
    "You’re free to run the crawler on any set of pages you like, but you can use this site if you want to compare your results to those in this chapter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!-- [ published at 2017-01-05 22:09:32 ] -->\n",
      "<html>\n",
      "<head>\n",
      "    <meta http-equiv=\"Co\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "##proxy = 'http://127.0.0.1:3128'\n",
    "##opener = urllib2.build_opener( urllib2.ProxyHandler({'http':proxy}) ) \n",
    "# urllib2.install_opener( opener )\n",
    "c=urllib2.urlopen('http://www.sina.com.cn')\n",
    "contents=c.read( )\n",
    "print contents[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using urllib2 and Beautiful Soup you can build a crawler that will take a list of URLs to index and crawl their links to find other pages to index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from BeautifulSoup import *\n",
    "from urlparse import urljoin\n",
    "# Create a list of words to ignore\n",
    "ignorewords=set(['the','of','to','and','a','in','is','it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting with a list of pages, do a breadth\n",
    "# first search to the given depth, indexing pages\n",
    "# as we go\n",
    "def crawl(self,pages,depth=2):\n",
    "  for i in range(depth):\n",
    "    newpages={}\n",
    "    for page in pages:\n",
    "      try:\n",
    "        c=urllib2.urlopen(page)\n",
    "      except:\n",
    "        print \"Could not open %s\" % page\n",
    "        continue\n",
    "      try:\n",
    "        soup=BeautifulSoup(c.read())\n",
    "        self.addtoindex(page,soup)\n",
    "\n",
    "        links=soup('a')\n",
    "        for link in links:\n",
    "          if ('href' in dict(link.attrs)):\n",
    "            url=urljoin(page,link['href'])\n",
    "            if url.find(\"'\")!=-1: continue\n",
    "            url=url.split('#')[0]  # remove location portion\n",
    "            if url[0:4]=='http' and not self.isindexed(url):\n",
    "              newpages[url]=1\n",
    "            linkText=self.gettextonly(link)\n",
    "            self.addlinkref(page,url,linkText)\n",
    "\n",
    "        self.dbcommit()\n",
    "      except:\n",
    "        print \"Could not parse page %s\" % page\n",
    "\n",
    "    pages=newpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import mySearchengine\n",
    "# reload(mySearchengine)\n",
    "# pagelist=['http://www.google.com']\n",
    "# crawler=mySearchengine.crawler('')\n",
    "# crawler.crawl(pagelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"bg s_ipt_wr\"><input id=\"kw\" name=\"wd\" class=\"s_ipt\" value=\"\" maxlength=\"255\" autocomplete=\"off\" /></span>\n",
      "<span class=\"bg s_btn_wr\"><input type=\"submit\" id=\"su\" value=\"百度一下\" class=\"bg s_btn\" /></span>\n",
      "<span class=\"tools\"><span id=\"mHolder\"><div id=\"mCon\"><span>输入法</span></div><ul id=\"mMenu\"><li><a href=\"javascript:;\" name=\"ime_hw\">手写</a></li><li><a href=\"javascript:;\" name=\"ime_py\">拼音</a></li><li class=\"ln\"></li><li><a href=\"javascript:;\" name=\"ime_cl\">关闭</a></li></ul></span></span>\n",
      "<span id=\"mHolder\"><div id=\"mCon\"><span>输入法</span></div><ul id=\"mMenu\"><li><a href=\"javascript:;\" name=\"ime_hw\">手写</a></li><li><a href=\"javascript:;\" name=\"ime_py\">拼音</a></li><li class=\"ln\"></li><li><a href=\"javascript:;\" name=\"ime_cl\">关闭</a></li></ul></span>\n",
      "<span>输入法</span>\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from BeautifulSoup import *\n",
    "from urlparse import urljoin\n",
    "# proxy = 'http://127.0.0.1:3128'\n",
    "# opener = urllib2.build_opener( urllib2.ProxyHandler({'http':proxy}) ) \n",
    "# urllib2.install_opener( opener )\n",
    "c=urllib2.urlopen('http://www.baidu.com')\n",
    "contents=c.read( )\n",
    "soup = BeautifulSoup(contents)\n",
    "links = soup('span')\n",
    "for i in range(len(links)):\n",
    "    print links[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
